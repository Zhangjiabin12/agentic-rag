# RAG å·¥å…·é›†

è¿™ä¸ªç›®å½•åŒ…å«äº†ä¸€ç³»åˆ—ç”¨äºå®ç°æ£€ç´¢å¢å¼ºç”Ÿæˆï¼ˆRetrieval-Augmented Generation, RAGï¼‰çš„å·¥å…·ã€‚æœ¬å·¥å…·é›†è®¾è®¡ç”¨äºå¤„ç†å’Œç®¡ç†å¤§è§„æ¨¡æ–‡æ¡£çŸ¥è¯†åº“ï¼Œæ”¯æŒå¤šç§æ–‡æ¡£æ ¼å¼å’Œå¤„ç†ç­–ç•¥ã€‚

## åŠŸèƒ½ç‰¹æ€§

- ğŸ“„ æ”¯æŒå¤šç§æ–‡ä»¶æ ¼å¼ï¼š
  - æ–‡æœ¬æ–‡ä»¶ï¼šTXTã€Markdown
  - åŠå…¬æ–‡æ¡£ï¼šPDFã€DOCXã€PPTX
  - ç»“æ„åŒ–æ•°æ®ï¼šCSVã€Excelã€JSON
  - ç½‘é¡µå†…å®¹ï¼šHTMLã€XML
- ğŸ§¹ æ–‡æœ¬æ¸…æ´—å’Œé¢„å¤„ç†
  - å»é™¤ç‰¹æ®Šå­—ç¬¦
  - ç»Ÿä¸€ç¼–ç æ ¼å¼
  - æ®µè½é‡ç»„
- âœ‚ï¸ æ™ºèƒ½æ–‡æœ¬åˆ†å—
  - åŸºäºè¯­ä¹‰çš„åˆ†å—
  - é‡å åˆ†å—ç­–ç•¥
  - è‡ªå®šä¹‰åˆ†å—è§„åˆ™
- ğŸ”¤ æ–‡æœ¬åµŒå…¥
  - æ”¯æŒå¤šç§åµŒå…¥æ¨¡å‹
  - æ‰¹é‡å¤„ç†ä¼˜åŒ–
  - ç¼“å­˜æœºåˆ¶
- ğŸ’¾ å‘é‡æ•°æ®åº“ç®¡ç†
  - é«˜æ•ˆå­˜å‚¨å’Œæ£€ç´¢
  - å…ƒæ•°æ®ç®¡ç†
  - å¢é‡æ›´æ–°æ”¯æŒ
- ğŸ¯ æ£€ç´¢ä¼˜åŒ–
  - è¯­ä¹‰é‡æ’åº
  - ç›¸å…³åº¦è¯„åˆ†
  - ä¸Šä¸‹æ–‡æ‰©å±•

## é…ç½®è¯´æ˜

åœ¨ `config.toml` ä¸­é…ç½® RAG ç›¸å…³å‚æ•°ï¼š

```toml
[rag]
vector_db_path = "./data/vector_db"
collection_name = "default"
embedding_model = "shibing624/text2vec-base-chinese"
rerank_model = "maidalun1020/bce-reranker-base_v1"
use_reranker = true
top_k = 5

[rag_server]
host = "127.0.0.1"
port = 65533
```

## ä½¿ç”¨æ–¹æ³•

### å¤„ç†å•ä¸ªæ–‡ä»¶

```python
from tools.rag_tools.document_processor import DocumentProcessor

# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
processor = DocumentProcessor(collection_name="my_collection")

# å¤„ç†å•ä¸ªæ–‡ä»¶
result = processor.process_file("path/to/file.txt", delete_existing=True)
```

### å¤„ç†å¤šä¸ªæ–‡ä»¶

```python
from tools.rag_tools.document_processor import DocumentProcessor

# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
processor = DocumentProcessor(collection_name="my_collection")

# å¤„ç†å¤šä¸ªæ–‡ä»¶
file_paths = ["path/to/file1.txt", "path/to/file2.pdf", "path/to/file3.docx"]
results = processor.process_files(file_paths, delete_existing=True, single_thread=True)
```

### å¤„ç†æ–‡ä»¶å¤¹

```python
from tools.rag_tools.document_processor import DocumentProcessor

# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
processor = DocumentProcessor(collection_name="my_collection")

# å¤„ç†æ–‡ä»¶å¤¹
results = processor.process_directory(
    "path/to/directory", 
    extensions=['.txt', '.md', '.pdf'], 
    delete_existing=True,
    single_thread=True
)
```

### æ£€ç´¢æ–‡æ¡£ï¼ˆæ¨èæ–¹å¼ï¼‰

æ¨èç›´æ¥ä½¿ç”¨VectorRetrieverè¿›è¡Œæ£€ç´¢ï¼Œè¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„èµ„æºæ¶ˆè€—ï¼š

```python
from tools.rag_tools.vector_retriever import VectorRetriever

# åˆ›å»ºæ£€ç´¢å™¨
retriever = VectorRetriever()

# æ£€ç´¢
query = "ä½ çš„æŸ¥è¯¢"
results = retriever.retrieve(query, collection_name="my_collection", top_k=3)

# è¾“å‡ºç»“æœ
for i, (doc, meta, score) in enumerate(zip(results["documents"], results["metadatas"], results["scores"])):
    print(f"ç»“æœ {i+1} (åˆ†æ•°: {score:.4f}):")
    print(f"æ¥æº: {meta.get('source', 'æœªçŸ¥')}")
    print(f"å†…å®¹: {doc}")
    print()
```

### æ£€ç´¢æ–‡æ¡£ï¼ˆé€šè¿‡DocumentProcessorï¼‰

ä¹Ÿå¯ä»¥é€šè¿‡DocumentProcessorè¿›è¡Œæ£€ç´¢ï¼Œä½†è¿™ä¼šåˆå§‹åŒ–æ›´å¤šç»„ä»¶ï¼š

```python
from tools.rag_tools.document_processor import DocumentProcessor

# åˆ›å»ºæ–‡æ¡£å¤„ç†å™¨
processor = DocumentProcessor(collection_name="my_collection")

# æ£€ç´¢
query = "ä½ çš„æŸ¥è¯¢"
results = processor.retrieve(query, top_k=3)

# è¾“å‡ºç»“æœ
for i, (doc, meta, score) in enumerate(zip(results["documents"], results["metadatas"], results["scores"])):
    print(f"ç»“æœ {i+1} (åˆ†æ•°: {score:.4f}):")
    print(f"æ¥æº: {meta.get('source', 'æœªçŸ¥')}")
    print(f"å†…å®¹: {doc}")
    print()
```

### ç›´æ¥åµŒå…¥æ–‡æœ¬

```python
from tools.rag_tools.text_embedder import TextEmbedder

# åˆ›å»ºåµŒå…¥å™¨
embedder = TextEmbedder()

# åµŒå…¥æ–‡æœ¬
text = "è¿™æ˜¯ä¸€æ®µç¤ºä¾‹æ–‡æœ¬"
metadata = {"source": "ç¤ºä¾‹æ¥æº"}
embedder.embed_text(text, collection_name="my_collection", metadata=metadata)
```

### å‘½ä»¤è¡Œä½¿ç”¨

ä¹Ÿå¯ä»¥é€šè¿‡å‘½ä»¤è¡Œä½¿ç”¨è¿™äº›å·¥å…·ï¼š

```bash
# å¤„ç†æ–‡ä»¶
python -m tools.rag_tools.document_processor --action process_file --input path/to/file.txt --collection_name my_collection

# å¤„ç†ç›®å½•
python -m tools.rag_tools.document_processor --action process_directory --input path/to/directory --collection_name my_collection --extensions .txt .md .pdf

# æ£€ç´¢æ–‡æ¡£
python -m tools.rag_tools.document_processor --action retrieve --query "ä½ çš„æŸ¥è¯¢" --collection_name my_collection

# åˆ—å‡ºæ‰€æœ‰é›†åˆ
python -m tools.rag_tools.document_processor --action list_collections

# åˆ é™¤é›†åˆ
python -m tools.rag_tools.document_processor --action delete_collection --collection_name my_collection
```

### ä½¿ç”¨REST APIæœåŠ¡

æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªåŸºäºFastAPIçš„REST APIæœåŠ¡ï¼Œå¯ä»¥é€šè¿‡HTTPè¯·æ±‚è¿›è¡Œæ–‡ä»¶å¤„ç†ã€å‘é‡åŒ–å’Œæ£€ç´¢ï¼š

#### å¯åŠ¨æœåŠ¡

```bash
python -m tools.rag_tools.rag_server --host 127.0.0.1 --port 8000
```

æœåŠ¡å¯åŠ¨åï¼Œå¯ä»¥é€šè¿‡æµè§ˆå™¨è®¿é—®APIæ–‡æ¡£ï¼š`http://127.0.0.1:8000/docs`

#### APIæ¥å£

1. **å¥åº·æ£€æŸ¥**
   ```
   GET /health
   ```

2. **åˆ—å‡ºæ‰€æœ‰é›†åˆ**
   ```
   GET /collections
   ```

3. **å¤„ç†å•ä¸ªæ–‡ä»¶**
   ```
   POST /process
   ```
   å‚æ•°ï¼š
   - `file`: æ–‡ä»¶ï¼ˆè¡¨å•æ•°æ®ï¼‰
   - `collection_name`: é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
   - `delete_existing`: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆï¼ˆå¯é€‰ï¼‰

4. **å¤„ç†ç›®å½•**
   ```
   POST /process_directory
   ```
   å‚æ•°ï¼š
   - `directory_path`: ç›®å½•è·¯å¾„ï¼ˆè¡¨å•æ•°æ®ï¼‰
   - `collection_name`: é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
   - `delete_existing`: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆï¼ˆå¯é€‰ï¼‰
   - `extensions`: æ–‡ä»¶æ‰©å±•ååˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
   - `single_thread`: æ˜¯å¦ä½¿ç”¨å•çº¿ç¨‹å¤„ç†ï¼ˆå¯é€‰ï¼‰

5. **å¤„ç†å¤šä¸ªæ–‡ä»¶**
   ```
   POST /process_files
   ```
   å‚æ•°ï¼š
   - `file_paths`: æ–‡ä»¶è·¯å¾„åˆ—è¡¨ï¼ˆJSONæ•°æ®ï¼‰
   - `collection_name`: é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
   - `delete_existing`: æ˜¯å¦åˆ é™¤å·²å­˜åœ¨çš„é›†åˆï¼ˆå¯é€‰ï¼‰
   - `single_thread`: æ˜¯å¦ä½¿ç”¨å•çº¿ç¨‹å¤„ç†ï¼ˆå¯é€‰ï¼‰

6. **ç›´æ¥åµŒå…¥æ–‡æœ¬**
   ```
   POST /embed_text
   ```
   å‚æ•°ï¼š
   - `text`: æ–‡æœ¬å†…å®¹ï¼ˆJSONæ•°æ®ï¼‰
   - `collection_name`: é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
   - `metadata`: å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰

7. **æ£€ç´¢**
   ```
   POST /retrieve
   ```
   å‚æ•°ï¼š
   - `query`: æŸ¥è¯¢æ–‡æœ¬
   - `collection_name`: é›†åˆåç§°ï¼ˆå¯é€‰ï¼‰
   - `top_k`: è¿”å›ç»“æœæ•°é‡ï¼ˆå¯é€‰ï¼‰

8. **åˆ é™¤é›†åˆ**
   ```
   DELETE /collection/{collection_name}
   ```
   å‚æ•°ï¼š
   - `collection_name`: é›†åˆåç§°ï¼ˆè·¯å¾„å‚æ•°ï¼‰

#### ä½¿ç”¨ç¤ºä¾‹

**Pythonç¤ºä¾‹**ï¼š

```python
import requests

# å¤„ç†æ–‡ä»¶
with open("path/to/file.txt", "rb") as f:
    response = requests.post(
        "http://127.0.0.1:8000/process",
        files={"file": f},
        data={"collection_name": "my_collection", "delete_existing": "true"}
    )
print(response.json())

# å¤„ç†ç›®å½•
response = requests.post(
    "http://127.0.0.1:8000/process_directory",
    data={
        "directory_path": "path/to/directory",
        "collection_name": "my_collection",
        "delete_existing": "true",
        "extensions": [".txt", ".md", ".pdf"],
        "single_thread": "true"
    }
)
print(response.json())

# æ£€ç´¢
response = requests.post(
    "http://127.0.0.1:8000/retrieve",
    json={"query": "ä½ çš„æŸ¥è¯¢", "collection_name": "my_collection", "top_k": 3}
)
results = response.json()
for i, (doc, meta, score) in enumerate(zip(results["documents"], results["metadatas"], results["scores"])):
    print(f"ç»“æœ {i+1} (åˆ†æ•°: {score:.4f}):")
    print(f"æ¥æº: {meta.get('source', 'æœªçŸ¥')}")
    print(f"å†…å®¹: {doc}")
    print()
```

**curlç¤ºä¾‹**ï¼š

```bash
# å¤„ç†æ–‡ä»¶
curl -X POST http://127.0.0.1:8000/process \
  -F "file=@path/to/file.txt" \
  -F "collection_name=my_collection" \
  -F "delete_existing=true"

# å¤„ç†ç›®å½•
curl -X POST http://127.0.0.1:8000/process_directory \
  -F "directory_path=path/to/directory" \
  -F "collection_name=my_collection" \
  -F "delete_existing=true" \
  -F "extensions=.txt" \
  -F "extensions=.md" \
  -F "extensions=.pdf" \
  -F "single_thread=true"

# æ£€ç´¢
curl -X POST http://127.0.0.1:8000/retrieve \
  -H "Content-Type: application/json" \
  -d '{"query": "ä½ çš„æŸ¥è¯¢", "collection_name": "my_collection", "top_k": 3}'
```

## æµ‹è¯•ç»“æœ

æˆ‘ä»¬è¿›è¡Œäº†å¤šä¸ªæ–‡ä»¶åµŒå…¥åˆ°åŒä¸€ä¸ªå‘é‡æ•°æ®åº“çš„æµ‹è¯•ï¼Œç»“æœè¡¨æ˜ï¼š

1. ç³»ç»Ÿèƒ½å¤ŸæˆåŠŸåœ°å°†å¤šä¸ªæ–‡ä»¶åµŒå…¥åˆ°åŒä¸€ä¸ªå‘é‡æ•°æ®åº“ã€‚
2. é€šç”¨æŸ¥è¯¢èƒ½å¤Ÿè¿”å›æ‰€æœ‰ç›¸å…³æ–‡æ¡£ã€‚
3. ç‰¹å®šæŸ¥è¯¢èƒ½å¤Ÿå‡†ç¡®åœ°è¿”å›æœ€ç›¸å…³çš„æ–‡æ¡£ï¼Œå¹¶ä¸”åˆ†æ•°å¾ˆé«˜ã€‚

æµ‹è¯•è„šæœ¬ä½äº `tools/rag_tools/test_multiple_files.py`ï¼Œå¯ä»¥è¿è¡Œè¯¥è„šæœ¬æŸ¥çœ‹æµ‹è¯•ç»“æœã€‚

## æ³¨æ„äº‹é¡¹

1. åœ¨å¤„ç†å¤šä¸ªæ–‡ä»¶æ—¶ï¼Œå»ºè®®ä½¿ç”¨ `single_thread=True` å‚æ•°ï¼Œé¿å…å¤šçº¿ç¨‹å†²çªã€‚
2. å¦‚æœéœ€è¦å¤„ç†å¤§é‡æ–‡ä»¶ï¼Œå»ºè®®ä½¿ç”¨æ‰¹å¤„ç†æ–¹å¼ï¼Œé¿å…å†…å­˜æº¢å‡ºã€‚
3. åœ¨åµŒå…¥æ–‡æœ¬æ—¶ï¼Œå»ºè®®ä½¿ç”¨å”¯ä¸€çš„ IDï¼Œé¿å… ID å†²çªã€‚
4. æ£€ç´¢æ–‡æ¡£æ—¶ï¼Œæ¨èç›´æ¥ä½¿ç”¨VectorRetrieverï¼Œè¿™æ ·å¯ä»¥é¿å…ä¸å¿…è¦çš„èµ„æºæ¶ˆè€—ã€‚
5. ä½¿ç”¨REST APIæœåŠ¡æ—¶ï¼Œæ¨¡å‹åªä¼šåŠ è½½ä¸€æ¬¡ï¼Œå¤§å¤§æé«˜äº†æ£€ç´¢æ•ˆç‡ã€‚

## æœ€ä½³å®è·µ

### æ–‡æ¡£å¤„ç†

1. æ–‡ä»¶é¢„å¤„ç†
   ```python
   from tools.rag_tools.document_processor import DocumentProcessor
   
   processor = DocumentProcessor(
       collection_name="knowledge_base",
       chunk_size=500,
       chunk_overlap=50,
       use_async=True  # å¯ç”¨å¼‚æ­¥å¤„ç†
   )
   
   # å¤„ç†å¤§å‹æ–‡æ¡£æ—¶ä½¿ç”¨å¤šçº¿ç¨‹
   processor.process_directory(
       "docs/",
       extensions=['.pdf', '.docx', '.txt'],
       single_thread=False,
       batch_size=10
   )
   ```

2. è‡ªå®šä¹‰åˆ†å—ç­–ç•¥
   ```python
   from tools.rag_tools.text_splitter import TextSplitter
   
   splitter = TextSplitter(
       chunk_size=500,
       chunk_overlap=50,
       separator="\n\n",  # æŒ‰æ®µè½åˆ†å‰²
       min_chunk_size=100  # æœ€å°åˆ†å—å¤§å°
   )
   ```

### å‘é‡æ£€ç´¢ä¼˜åŒ–

1. ä½¿ç”¨é‡æ’åºæé«˜ç²¾åº¦
   ```python
   from tools.rag_tools.vector_retriever import VectorRetriever
   
   retriever = VectorRetriever(
       use_reranker=True,
       rerank_top_k=10,
       use_cache=True  # å¯ç”¨ç¼“å­˜
   )
   
   results = retriever.retrieve(
       query="æŸ¥è¯¢å†…å®¹",
       collection_name="knowledge_base",
       top_k=3
   )
   ```

2. æ‰¹é‡æ£€ç´¢ä¼˜åŒ–
   ```python
   # æ‰¹é‡æŸ¥è¯¢
   queries = ["é—®é¢˜1", "é—®é¢˜2", "é—®é¢˜3"]
   results = retriever.batch_retrieve(
       queries,
       collection_name="knowledge_base",
       top_k=3,
       batch_size=5  # æ§åˆ¶æ‰¹å¤„ç†å¤§å°
   )
   ```

### REST API æœ€ä½³å®è·µ

1. æ–‡ä»¶ä¸Šä¼ ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
   ```python
   import aiohttp
   import asyncio
   
   async def upload_file(file_path: str, collection_name: str):
       async with aiohttp.ClientSession() as session:
           with open(file_path, "rb") as f:
               response = await session.post(
                   "http://localhost:8000/process",
                   data={
                       "collection_name": collection_name,
                       "chunk_size": 500,
                       "chunk_overlap": 50,
                       "use_async": "true"
                   },
                   files={"file": f}
               )
           return await response.json()
   ```

2. æ‰¹é‡å¤„ç†ï¼ˆæ”¯æŒå¼‚æ­¥ï¼‰
   ```python
   async def process_directory(dir_path: str, collection_name: str):
       async with aiohttp.ClientSession() as session:
           response = await session.post(
               "http://localhost:8000/process_directory",
               json={
                   "directory_path": dir_path,
                   "collection_name": collection_name,
                   "extensions": [".pdf", ".docx", ".txt"],
                   "single_thread": False,
                   "batch_size": 10,
                   "use_async": True
               }
           )
           return await response.json()
   ```

## æ€§èƒ½ä¼˜åŒ–å»ºè®®

1. æ–‡æ¡£å¤„ç†
   - ä½¿ç”¨å¼‚æ­¥å¤„ç†å¤§æ–‡ä»¶
   - å¯ç”¨å¤šçº¿ç¨‹å¤„ç†å¤§é‡æ–‡ä»¶
   - ä½¿ç”¨é€‚å½“çš„åˆ†å—å¤§å°ï¼ˆæ¨è300-500å­—ï¼‰
   - æ ¹æ®æ–‡æ¡£ç±»å‹é€‰æ‹©åˆé€‚çš„åˆ†å—ç­–ç•¥
   - å¯ç”¨æ‰¹å¤„ç†æ¨¡å¼
   - ä½¿ç”¨æµå¼å¤„ç†é¿å…å†…å­˜æº¢å‡º

2. å‘é‡æ£€ç´¢
   - ä½¿ç”¨å¼‚æ­¥æ‰¹å¤„ç†
   - å¯ç”¨ç»“æœç¼“å­˜
   - é€‚å½“è°ƒæ•´top_kå‚æ•°
   - é’ˆå¯¹ç‰¹å®šåœºæ™¯ä¼˜åŒ–é‡æ’åº
   - ä½¿ç”¨å‘é‡æ•°æ®åº“ç´¢å¼•
   - å®šæœŸä¼˜åŒ–å‘é‡æ•°æ®åº“

3. ç³»ç»Ÿé…ç½®
   - åˆç†è®¾ç½®å¹¶å‘é™åˆ¶
   - ä¼˜åŒ–å†…å­˜ä½¿ç”¨
   - å®šæœŸæ¸…ç†ç¼“å­˜
   - ç›‘æ§ç³»ç»Ÿæ€§èƒ½
   - ä½¿ç”¨è¿æ¥æ± 
   - å¯ç”¨å‹ç¼©ä¼ è¾“

## æ•…éšœæ’é™¤

1. å†…å­˜ä½¿ç”¨è¿‡é«˜
   - å‡å°æ‰¹å¤„ç†å¤§å°
   - ä½¿ç”¨æµå¼å¤„ç†
   - æ¸…ç†å‘é‡ç¼“å­˜
   - ä½¿ç”¨å†…å­˜ç›‘æ§
   - å®šæœŸé‡Šæ”¾èµ„æº

2. å¤„ç†é€Ÿåº¦æ…¢
   - æ£€æŸ¥æ–‡ä»¶å¤§å°
   - ä¼˜åŒ–åˆ†å—ç­–ç•¥
   - è°ƒæ•´å¹¶å‘å‚æ•°
   - ä½¿ç”¨æ€§èƒ½åˆ†æå·¥å…·
   - ä¼˜åŒ–æ•°æ®åº“æŸ¥è¯¢

3. æ£€ç´¢è´¨é‡é—®é¢˜
   - éªŒè¯æ–‡æ¡£åˆ†å—
   - è°ƒæ•´é‡æ’åºå‚æ•°
   - æ£€æŸ¥åµŒå…¥æ¨¡å‹
   - åˆ†æç›¸ä¼¼åº¦åˆ†æ•°
   - ä¼˜åŒ–æç¤ºè¯

## å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒæ›´å¤šæ–‡ä»¶æ ¼å¼
- [ ] æ·»åŠ å¢é‡æ›´æ–°åŠŸèƒ½
- [ ] ä¼˜åŒ–æ£€ç´¢ç®—æ³•
- [ ] æ·»åŠ æ–‡æ¡£è´¨é‡è¯„ä¼°
- [ ] æ”¯æŒåˆ†å¸ƒå¼å¤„ç†
- [ ] æ·»åŠ å‘é‡æ•°æ®åº“è¿ç§»å·¥å…·
- [ ] å®ç°è‡ªåŠ¨åŒ–æµ‹è¯•
- [ ] ä¼˜åŒ–å†…å­˜ç®¡ç†
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§
- [ ] æ”¯æŒå¤šè¯­è¨€å¤„ç†

## æ›´æ–°æ—¥å¿—

### v1.0.0
- åˆå§‹ç‰ˆæœ¬å‘å¸ƒ
- æ”¯æŒåŸºæœ¬æ–‡æ¡£å¤„ç†
- å®ç°å‘é‡æ£€ç´¢åŠŸèƒ½

### v1.1.0
- æ·»åŠ é‡æ’åºåŠŸèƒ½
- ä¼˜åŒ–æ–‡æœ¬åˆ†å—
- æ”¹è¿›APIæ¥å£
- æ·»åŠ å¼‚æ­¥å¤„ç†æ”¯æŒ
- ä¼˜åŒ–å†…å­˜ä½¿ç”¨
- æ·»åŠ ç¼“å­˜æœºåˆ¶

## è´¡çŒ®æŒ‡å—

1. Fork é¡¹ç›®
2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯
3. æäº¤æ›´æ”¹
4. å‘èµ· Pull Request

## è®¸å¯è¯

MIT License 